{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQpC4ElvTRx3"
   },
   "source": [
    "<font color=\"#de3023\"><h1><b>REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>\n",
    "\n",
    "To copy the notebook, go to File and click create \"Save a copy to ...\" and work on that copy.\n",
    "\n",
    "Please submit a pdf of the page of your notebook (Ctrl + p on the page, save as pdf, and submit that pdf) on gradescope.\n",
    "\n",
    "Please remember to assign pages to the appropriate questions. Not doing so will result in the deduction of points. Please submit a **pdf** version of the colab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfFHyJTPcQFy"
   },
   "source": [
    "# **Homework 3: Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJYifLRUbNEo"
   },
   "source": [
    "# Basic Import Statements\n",
    "Feel free to import as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMRr8jXruk3W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# If need be, you can add more import statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaOERvxba7Qw"
   },
   "source": [
    "# Loading regression Data\n",
    "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>\n",
    "We will be using the same California Housing Data for Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVCxSdrZa8hg"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "california_housing = fetch_california_housing( return_X_y=True, as_frame=True)\n",
    "X = california_housing[0]\n",
    "y = california_housing[1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# np.random.seed(42)\n",
    "sc=StandardScaler()\n",
    "X_transform=sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpfHf6g6b1oD"
   },
   "source": [
    "## Question 0 Part (a)\n",
    "Do you have confusions or questions about the previous lectures?  (This is optional to answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBxMiyi0b_eu"
   },
   "source": [
    "(Answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q6_cb2YcBkn"
   },
   "source": [
    "## Question 0 Part (b)\n",
    "Any suggestions or thoughts about the course? (This is optional to answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g620sJBBcHxq"
   },
   "source": [
    "(Answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZdouTj5Mlhd"
   },
   "source": [
    "## Question 1: Gradient Descent for OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrNdf5b9IBvd"
   },
   "source": [
    "### Part 1 - Simple Linear Regression (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkG-0WoxH-eB"
   },
   "source": [
    "Remember that in the simple linear regression we want to find $(\\alpha, \\beta)$ that minimize the sum of the square of the errors:\n",
    "\n",
    "$$(\\widehat{\\alpha}, \\widehat{\\beta}) = argmin_{(\\alpha, \\beta)} L(\\alpha, \\beta) = argmin_{(\\alpha, \\beta)} \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\alpha - \\beta x_i)^2$$\n",
    "\n",
    "We can approximate the solution of the simple linear regression using gradient descent in the following way:\n",
    "\n",
    "\n",
    "**Inputs:** $X, Y,\\alpha_0, \\beta_0, \\eta, \\epsilon$\n",
    "\n",
    "**Setp 1:** For $k =0$, initialize $\\alpha_k$ and $\\beta_k$:\n",
    "\n",
    "$$\\alpha_k = \\alpha_0$$\n",
    "$$\\beta_k = \\beta_0$$\n",
    "\n",
    "**Step 2:** For $k \\geq 1$ repeat until $|\\alpha_k - \\alpha_{k-1}| + |\\beta_k - \\beta_{k-1}| < \\epsilon$:\n",
    "\n",
    "- **Step 2 a):** Update $\\alpha_k$ and $\\beta_k$ according to the rule:\n",
    "\n",
    "$$\\alpha_k = \\alpha_{k-1} - \\eta \\frac{dL}{d\\alpha} = \\alpha_{k-1} (1-\\eta) + \\frac{\\eta}{n}\\sum_{i=1}^n y_i - \\beta_{k-1}\\frac{\\eta}{n}\\sum_{i=1}^n x_i$$\n",
    "\n",
    "$$\\beta_k = \\beta_{k-1} - \\eta \\frac{dL}{d\\beta} = \\beta_{k-1} + \\frac{\\eta}{n}\\sum_{i=1}^n x_i y_i - \\alpha_{k-1}\\frac{\\eta}{n}\\sum_{i=1}^n x_i - \\beta_{k-1}\\frac{\\eta}{n}\\sum_{i=1}^n x_i^2$$\n",
    "\n",
    "or in vector notation\n",
    "\n",
    "$$(\\alpha_k, \\beta_k) = (\\alpha_{k-1}, \\beta_{k-1}) - \\eta \\nabla L $$\n",
    "\n",
    "- **Step 2 b):** Calculate and store loss of current interation:\n",
    "$$L_k = \\frac{1}{2n}\\sum_{i=1}^n (y_i - \\alpha_k - \\beta_k x_i)^2$$\n",
    "\n",
    "**Step 3:** Plot the training loss across all iterations $(L_0, L_1,...)$\n",
    "\n",
    "**Output:** $(\\alpha_k, \\beta_k)$, $L_k$\n",
    "\n",
    "Write an algorithm for gradient descent which will output  $(\\widehat{\\alpha}, \\widehat{\\beta})$  and the training loss of the model using  $(\\widehat{\\alpha}, \\widehat{\\beta})$  with a given  $X$  vector with dimensions  $nx1$ ,  $Y$  vector with dimensions  $nx1$ ,  $\\eta$  learning rate,  $(\\alpha_0,\\beta_0)$  initailization for  $(\\widehat{\\alpha}, \\widehat{\\beta})$, and  $\\epsilon$  convergence condition. This algorithm should also plot the losses across all iterations.\n",
    "\n",
    "There are two ways to define a convergence condition for gradient descent:\n",
    "\n",
    "*   check if the changes made to $\\alpha_k$ and $\\beta_k$ are smaller than epsilon.\n",
    "\n",
    "$$|\\alpha_k - \\alpha_{k-1}| + |\\beta_k - \\beta_{k-1}| < \\epsilon$$\n",
    "\n",
    "*   check if the change made to the loss is smaller than epsilon.\n",
    "\n",
    "$$|L_k - L_{k-1}| < \\epsilon$$\n",
    "\n",
    "You can implement either for the rest of HW3.\n",
    "\n",
    "Implement this using one loop for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrIOs5PrORFe"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# [Question 1 Part 1: 20 points]                                            #\n",
    "# TODO:                                                                     #\n",
    "# Implement Gradient Descent for Simple Linear Regression using the gradient#\n",
    "# formula shown above.                                                      #\n",
    "# Input: X of shape (n,), y of shape (n,), eta,                             #\n",
    "#        initial_w of shape ((p + 1), ), epsilon                            #\n",
    "# Output: alpha, beta and Training loss using those values.                 #\n",
    "# Also plot your losses across all iterations                               #\n",
    "# ONLY use numpy for this section! Use of scikit-learn will give you 0 points\n",
    "#############################################################################\n",
    "\n",
    "def simp_lin_grad_descent(X, y, eta, initial_alpha, initial_beta, epsilon):\n",
    "  #############################################################################\n",
    "  #                              START OF YOUR CODE                           #\n",
    "  #############################################################################\n",
    "  n = X.shape[0]\n",
    "  alpha, beta = None, None\n",
    "  loss = 0\n",
    "  # Replace \"...\" statement with your code for the iterative w update\n",
    "  #...\n",
    "  # Insert Code for plots here\n",
    "  #...\n",
    "  return alpha, beta, losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZXsEQZkT5Qt"
   },
   "source": [
    "### Test cases\n",
    "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-vfIZAhSjEV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.rand(100)\n",
    "alpha = np.random.rand(1)\n",
    "beta = np.random.rand(1)\n",
    "y_ = alpha + x * beta + np.random.rand(100)/10\n",
    "\n",
    "eta, initial_alpha, initial_beta, epsilon = 0.1, 0, 0, 0.001\n",
    "alpha_hat, beta_hat, loss = simp_lin_grad_descent(x, y_, eta, initial_alpha, initial_beta, epsilon)\n",
    "\n",
    "print(\"The training loss is {}\".format(loss))\n",
    "print(\"The weights are {}\".format([alpha_hat, beta_hat]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cza4CWxfFA-U"
   },
   "source": [
    "### Part 2 - Batch Gradient Descent (40 points)\n",
    "We implemented Gradient Descent for simple linear regression but what about multiple linear regression?\n",
    "\n",
    "In general, we can apply gradient descent algorithm in the following way:\n",
    "\n",
    "$$w_k = w_{k-1} - \\eta \\nabla L $$\n",
    "\n",
    "Where $w$ is the weight vector of the model, $\\eta$ is the learning rate, $L$ is the loss function and $\\nabla L$ is the corresponding gradient.\n",
    "\n",
    "In particular, for multiple linear regression we have:\n",
    "\n",
    "$$L(w) = \\frac{1}{2n}\\sum_{i=1}^n (y_i - x_i^T w)^2 = \\frac{1}{2n}||Y - X w||^2$$\n",
    "\n",
    "$$\\nabla L = \\frac{1}{n} X^T(Xw - Y)$$\n",
    "\n",
    "So the gradient descent for  multiple linear regression would be\n",
    "\n",
    "**Inputs:** $X, Y,\\alpha_0, \\beta_0, \\eta, \\epsilon$\n",
    "\n",
    "**Step 1:** For $k=0$ initialize $w_k$\n",
    "\n",
    "$$w_k = w_0$$\n",
    "\n",
    "**Step 2:** For  $k\\geq 1$  repeat until $||w_k - w_{k-1}|| < \\epsilon$:\n",
    "\n",
    "- **Step 2 a) :** Update the weight vector $w_k$ according to the rule:\n",
    "$$w_k = w_{k-1} - \\eta \\frac{1}{n} X^T(Xw_{k-1} - Y) $$\n",
    "\n",
    "- **Step 2 b):** Calculate and store loss of current interation:\n",
    "$$L_k = \\frac{1}{2n}||Y - X w_k||^2$$\n",
    "\n",
    "**Step 3:** Plot the training loss across all iterations $(L_0, L_1,...)$\n",
    "\n",
    "**Output:** $w_k$, $L_k$\n",
    "\n",
    "Write an algorithm for gradient descent which will output $\\widehat{w}$ and the training loss of the model using $\\widehat{w}$ with a given $X$ matrix with dimensions $n \\times p$, $Y$ vector with dimensions $n \\times 1$, $\\eta$ learning rate, $w_0$ initailization for $w$, and $\\epsilon$ convergence condition. This algorithm should also plot the losses across all iterations.\n",
    "\n",
    "There are two ways to define a convergence condition for gradient descent:\n",
    "\n",
    "*   check if the changes made to $\\alpha_k$ and $\\beta_k$ are smaller than epsilon.\n",
    "\n",
    "$$||w_k - w_{k-1}|| < \\epsilon$$\n",
    "\n",
    "*   check if the change made to the loss is smaller than epsilon.\n",
    "\n",
    "$$|L_k - L_{k-1}| < \\epsilon$$\n",
    "\n",
    "You can implement either for the rest of HW3.\n",
    "\n",
    "Implement this using one loop for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbWrwp-mFsyV"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# [Question 1 Part 2: 40 points]                                            #\n",
    "# TODO:                                                                     #\n",
    "# Implement Gradient Descent for Linear Regression using the gradient.      #\n",
    "# formula derived in class.                                                 #\n",
    "# Input: X of shape (n, p), y of shape (n,), eta,                           #\n",
    "#        initial_w of shape ((p + 1), ), epsilon                            #\n",
    "# Output: w of shape ((p + 1), ) and Training loss using that weight.       #\n",
    "# Also plot your losses across all iterations                               #\n",
    "# ONLY use numpy for this section! Use of scikit-learn will give you 0 points\n",
    "#############################################################################\n",
    "\n",
    "def lin_grad_descent(X, y, eta, initial_w, epsilon):\n",
    "  #############################################################################\n",
    "  #                              START OF YOUR CODE                           #\n",
    "  #############################################################################\n",
    "  n, p = X.shape\n",
    "  ones = np.ones((n, 1))\n",
    "  new_X = np.hstack((ones, X))\n",
    "  new_y = np.array(y).reshape((n, 1))\n",
    "  # Replace \"...\" statement with your code\n",
    "  ...\n",
    "\n",
    "  # Insert Code for plots here\n",
    "  ...\n",
    "  #############################################################################\n",
    "  #                              END OF YOUR CODE                             #\n",
    "  #############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7Kp1N_RIq-N"
   },
   "source": [
    "### Test cases\n",
    "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Oc0b0MwQPbW"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "new_p = X.shape[1] + 1\n",
    "w_graddescent , loss = lin_grad_descent(X_transform, y_train, eta = 0.01, initial_w =np.random.randn(new_p,1), epsilon = 0.001)\n",
    "pred_y_test = np.hstack((np.ones((X_test.shape[0], 1)), sc.transform(X_test))) @ w_graddescent\n",
    "new_y = np.array(y_test).reshape((y_test.shape[0], 1))\n",
    "print(\"The training loss is {}\".format(loss))\n",
    "print(\"The test loss is {}\".format(np.mean(np.square(new_y-pred_y_test))))\n",
    "print(\"The weights are {}\".format(w_graddescent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncnnwmQiUny4"
   },
   "source": [
    "### Part 3 - Mini batch gradient descent (40 points)\n",
    "\n",
    "In the mini batch gradient descent, we apply gradient descent to a subset of the data that we call Batch with size $|B|$ at each iteration. The pseudo code is the following:\n",
    "\n",
    "**Inputs:** $X, Y,\\alpha_0, \\beta_0, \\eta, \\epsilon, |B|$\n",
    "\n",
    "Step 0: set n  = number of rows in $X$ and calculate the number of batches as $m = n/|B|$ (if $n$ is not divisible by |B|, take the smallest integer greater than or equal to $n/|B|$)\n",
    "\n",
    "**Step 1:** For $k=0$ initialize $w_k$\n",
    "\n",
    "$$w_k = w_0$$\n",
    "\n",
    "**Step 2:** Repeat until $||w_k - w_{k-1}|| < \\epsilon$:\n",
    "\n",
    "- **Step 2 a):** Split the data randomly into $m$ batches: $B_1,B_2,...,B_{m}$. For each batch, we will have the corresponding $X_{B_1}, X_{B_2},...,X_{B_m}$ and $Y_{B_1}, Y_{B_2},...,Y_{B_m}$.\n",
    "\n",
    "- **Step 2 b):** For each batch $j$ in $\\{1,2,...,m\\}$ apply gradient descent:\n",
    "\n",
    "  - **Step 2 b.1):** Update the weight vector $w_k$ according to the rule:\n",
    "      \n",
    "      $$w_k = w_{k-1} - \\eta \\frac{1}{|B|} X_{B_j}^T(X_{B_j}w_{k-1} - Y_{B_j}) $$\n",
    "\n",
    "- **Step 2 c):** After applying gradient descent to all the batches, calculate and store loss for the whole data:\n",
    "\n",
    "$$L_k = \\frac{1}{2n}||Y - X w_k||^2$$\n",
    "\n",
    "**Step 3:** Plot the training loss across all iterations $(L_0, L_1,...)$\n",
    "\n",
    "**Output:** $w_k$, $L_k$\n",
    "\n",
    "Write an algorithm for mini batch gradient descent which will output $\\widehat{w}$ and the training loss of the model using $\\widehat{w}$ with a given $X$ matrix with dimensions $n \\times p$, $Y$ vector with dimensions $n \\times 1$, $\\eta$ learning rate, $w_0$ initailization for $w$, $|B|$ minibatch size, and $\\epsilon$ convergence condition. This algorithm should also plot the losses across all iterations (similar to lab)\n",
    "\n",
    "Implement this using two loop for full credit.\n",
    "\n",
    "**Hint:** It might be helpful to use the np.random.shuffle() function to shuffle the data (this has been implemented for you) and then use a for loop with $i$ going from 1 to $n / |B|$ and make a gradient descent step on the data from $i * |B|$th row to $(i + 1) * |B|$th row in the shuffled data set. If $|B|$ is not exactly divisible by the sample size, make the last minibatch of size $n \\% |B|$ and change the update rule accordingly for the last minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WK3Gy49YU5tV"
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# [Question 1 Part 3: 40 points]                                            #\n",
    "# TODO:                                                                     #\n",
    "# Implement Mini batch Gradient Descent for Linear Regression using         #\n",
    "# the gradient formula derived in class.                                    #\n",
    "# Input: X of shape (n, p), y of shape (n,), eta,                           #\n",
    "#        initial_w of shape ((p + 1), ), epsilon, batch_size                #\n",
    "# Output: w of shape ((p + 1), ) and Training loss using that weight.       #\n",
    "# Also plot your losses across all iterations                               #\n",
    "#                                                                           #\n",
    "# ONLY use numpy for this section! Use of scikit-learn will give you 0 points\n",
    "#############################################################################\n",
    "\n",
    "def mini_batch_lin_grad_descent(X, y, eta, initial_w, epsilon, batch_size):\n",
    "  #############################################################################\n",
    "  #                              START OF YOUR CODE                           #\n",
    "  #############################################################################\n",
    "  n, p = X.shape\n",
    "  ones = np.ones((n, 1))\n",
    "  new_X = np.hstack((ones, X))\n",
    "  new_y = np.array(y).reshape((n, 1))\n",
    "  # Replace \"...\" statement with your code\n",
    "  ...\n",
    "\n",
    "  # Insert code for the plot here\n",
    "  ...\n",
    "  #############################################################################\n",
    "  #                              END OF YOUR CODE                             #\n",
    "  #############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol_SNUDNav_6"
   },
   "source": [
    "### Test cases\n",
    "<font color=\"#de3023\"><h6><b>DO NOT MAKE EDITS TO THIS SECTION</b></h6></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46vFN18FataS"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "w_mini, loss = mini_batch_lin_grad_descent(X_transform, y_train, eta = 0.0001, initial_w =np.random.randn(new_p,1), epsilon = 0.01, batch_size = 100)\n",
    "pred_y_test = np.hstack((np.ones((X_test.shape[0], 1)), sc.transform(X_test))) @ w_mini\n",
    "print(\"The training loss is {}\".format(loss))\n",
    "print(\"The test loss is {}\".format(np.mean(np.square(new_y-pred_y_test))))\n",
    "print(\"The weights are {}\".format(w_mini))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1KiYHGcL7h4vDPdK9_02jvmZidSXQyT5e",
     "timestamp": 1694273678586
    },
    {
     "file_id": "1JHWEZDb4oCYzZoPC8qDyP-5fcMtfht8k",
     "timestamp": 1693694574067
    },
    {
     "file_id": "1rY5OYXZNAZaGLRCIO6oO-Qzy18d3TTao",
     "timestamp": 1693584941739
    },
    {
     "file_id": "1lVVr-pv_5Kzb4eWay93BqSpGcUis4eYa",
     "timestamp": 1693431473318
    },
    {
     "file_id": "1Ql1RaSNJdvklslNt8GQDTSEFUtzwPRu_",
     "timestamp": 1675897093857
    },
    {
     "file_id": "1MWMo1-YwC1dYvTwOQIdhxkIEcEaex94v",
     "timestamp": 1674183123083
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
